"""
AI Assistant Module pour CYBER FORGE SCAN
Utilise Ollama pour fournir une assistance conversationnelle lÃ©gÃ¨re
"""

import requests
import json
import sys
import subprocess
from typing import Optional

def check_and_install_dependencies():
    """VÃ©rifie et installe requests si nÃ©cessaire"""
    try:
        import requests
        print("âœ… requests est dÃ©jÃ  installÃ©")
        return True
    except ImportError:
        print("âš ï¸  requests n'est pas installÃ©")
        print("\nğŸ“¦ Installation en cours...")
        print("â³ Merci de patienter...\n")
        
        try:
            subprocess.check_call([
                sys.executable, 
                "-m", 
                "pip", 
                "install", 
                "requests"
            ])
            print("\nâœ… requests installÃ© avec succÃ¨s!")
            return True
        except subprocess.CalledProcessError as e:
            print(f"\nâŒ Erreur lors de l'installation: {e}")
            print("\nğŸ’¡ Essaye manuellement: py -m pip install requests")
            return False

# VÃ©rifier les dÃ©pendances
print("ğŸ” VÃ©rification des dÃ©pendances...")
if not check_and_install_dependencies():
    print("\nâš ï¸  Le module ne peut pas fonctionner sans requests")
    input("\nAppuie sur EntrÃ©e pour quitter...")
    sys.exit(1)

import requests


class CyberForgeAssistant:
    """Assistant IA local pour rÃ©pondre aux questions des utilisateurs"""
    
    def __init__(self, model: str = "phi3:mini", base_url: str = "http://localhost:11434"):
        """
        Initialise l'assistant
        
        Args:
            model: Nom du modÃ¨le Ollama (phi3:mini, gpt-oss:20b-cloud, etc.)
            base_url: URL de l'API Ollama locale
        """
        self.model = model
        self.base_url = base_url
        self.conversation_history = []
        
    def check_ollama_status(self) -> bool:
        """VÃ©rifie si Ollama est en cours d'exÃ©cution"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=2)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
    
    def get_available_models(self) -> list:
        """Liste les modÃ¨les Ollama disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
            return []
        except:
            return []
    
    def ask(self, question: str, context: Optional[str] = None, stream: bool = False) -> str:
        """
        Pose une question Ã  l'assistant
        
        Args:
            question: Question de l'utilisateur
            context: Contexte optionnel (infos sur CYBER FORGE SCAN)
            stream: Afficher la rÃ©ponse en temps rÃ©el (True/False)
        
        Returns:
            RÃ©ponse de l'assistant
        """
        # VÃ©rifie qu'Ollama tourne
        if not self.check_ollama_status():
            return "âŒ Erreur: Ollama n'est pas dÃ©marrÃ©. Lance 'ollama serve' dans un terminal."
        
        # PrÃ©pare le prompt avec contexte si fourni
        prompt = question
        if context:
            prompt = f"Contexte: {context}\n\nQuestion: {question}"
        
        # Ajoute Ã  l'historique
        self.conversation_history.append({"role": "user", "content": prompt})
        
        try:
            # Appel API Ollama
            if stream:
                return self._ask_stream(prompt)
            else:
                return self._ask_normal()
                
        except requests.exceptions.Timeout:
            return "â±ï¸ Timeout: La requÃªte a pris trop de temps. RÃ©essaye."
        except requests.exceptions.RequestException as e:
            return f"âŒ Erreur de connexion: {str(e)}"
        except Exception as e:
            return f"âŒ Erreur inattendue: {str(e)}"
    
    def _ask_normal(self) -> str:
        """RequÃªte normale sans streaming"""
        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": self.conversation_history,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            assistant_response = result["message"]["content"]
            
            # Ajoute la rÃ©ponse Ã  l'historique
            self.conversation_history.append({
                "role": "assistant", 
                "content": assistant_response
            })
            
            return assistant_response
        else:
            return f"âŒ Erreur API Ollama: {response.status_code}"
    
    def _ask_stream(self, prompt: str) -> str:
        """RequÃªte avec streaming (affichage en temps rÃ©el)"""
        print("\nğŸ¤– Assistant: ", end="", flush=True)
        
        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": self.conversation_history,
                "stream": True
            },
            stream=True,
            timeout=60
        )
        
        full_response = ""
        
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line)
                    if "message" in data:
                        chunk = data["message"].get("content", "")
                        print(chunk, end="", flush=True)
                        full_response += chunk
                except json.JSONDecodeError:
                    continue
        
        print()  # Nouvelle ligne aprÃ¨s le streaming
        
        # Ajoute la rÃ©ponse complÃ¨te Ã  l'historique
        self.conversation_history.append({
            "role": "assistant", 
            "content": full_response
        })
        
        return full_response
    
    def reset_conversation(self):
        """RÃ©initialise l'historique de conversation"""
        self.conversation_history = []
        print("ğŸ”„ Conversation rÃ©initialisÃ©e!")
    
    def change_model(self, new_model: str):
        """Change le modÃ¨le utilisÃ©"""
        available = self.get_available_models()
        if new_model in available:
            self.model = new_model
            print(f"âœ… ModÃ¨le changÃ© pour: {new_model}")
            self.reset_conversation()
        else:
            print(f"âŒ ModÃ¨le {new_model} non disponible")
            print(f"ğŸ“‹ ModÃ¨les disponibles: {', '.join(available)}")


# ============ EXEMPLE D'UTILISATION ============

def demo_assistant():
    """Fonction de dÃ©monstration de l'assistant"""
    print("\nğŸ¤– CYBER FORGE SCAN - Assistant NOVA IA")
    print("=" * 50)
    
    # Initialise l'assistant
    assistant = CyberForgeAssistant(model="phi3:mini")
    
    # VÃ©rifie le statut
    if not assistant.check_ollama_status():
        print("\nâŒ Ollama n'est pas dÃ©marrÃ©!")
        print("ğŸ’¡ Lance dans un terminal: ollama serve")
        input("\nAppuie sur EntrÃ©e pour quitter...")
        return
    
    # Liste les modÃ¨les disponibles
    models = assistant.get_available_models()
    print(f"\nâœ… Ollama est en ligne!")
    print("\nDemmarage de NOVA!")
    print(f"ğŸ“‹ ModÃ¨les disponibles: {', '.join(models)}")
    print(f"ğŸ¯ ModÃ¨le actuel: {assistant.model}")
    
    print("\nğŸ’¬ Commandes spÃ©ciales:")
    print("  - 'exit' ou 'quit' : Quitter NOVA")
    print("  - 'reset' : Nouvelle conversation avec NOVA")
    print("  - 'model' : Changer de modÃ¨le")
    print("  - 'stream on/off' : Activer/dÃ©sactiver le streaming")
    
    # Contexte sur l'outil
    context = """
    CYBER FORGE SCAN est un outil de cybersÃ©curitÃ© Python qui permet de:
    - Scanner les vulnÃ©rabilitÃ©s
    - TÃ©lÃ©charger des vidÃ©os YouTube
    - Convertir des documents en PDF
    - Tester la vitesse Internet
    - Utiliser une IA locale via Ollama
    
    L'utilisateur peut avoir des questions sur l'utilisation, les rÃ©sultats,
    ou des concepts de cybersÃ©curitÃ© en gÃ©nÃ©ral.
    """
    
    stream_mode = False
    
    print("\n" + "=" * 50)
    print("ğŸš€ L'assistant NOVA est prÃªt!")
    print("=" * 50 + "\n")
    
    while True:
        user_input = input("Toi: ").strip()
        
        if not user_input:
            continue
        
        # Commandes spÃ©ciales
        if user_input.lower() in ['exit', 'quit']:
            print("\nğŸ‘‹ Ã€ bientÃ´t!")
            break
        
        if user_input.lower() == 'reset':
            assistant.reset_conversation()
            continue
        
        if user_input.lower() == 'model':
            print(f"\nğŸ“‹ ModÃ¨les disponibles: {', '.join(models)}")
            new_model = input("Nouveau modÃ¨le: ").strip()
            if new_model:
                assistant.change_model(new_model)
            continue
        
        if user_input.lower() == 'stream on':
            stream_mode = True
            print("âœ… Streaming activÃ©")
            continue
        
        if user_input.lower() == 'stream off':
            stream_mode = False
            print("âœ… Streaming dÃ©sactivÃ©")
            continue
        
        # Question normale
        if not stream_mode:
            print("\nğŸ¤– Assistant NOVA: ", end="", flush=True)
        
        response = assistant.ask(user_input, context=context, stream=stream_mode)
        
        if not stream_mode:
            print(response + "\n")


if __name__ == "__main__":
    demo_assistant()